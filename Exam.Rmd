---
title: "Predictive Models Take Home Exam"
author: "Hitesh Prabhu"
date: "July 27, 2016"
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      warning=FALSE, message=FALSE)
```

## Question 1 - Chapter 2  |  Problem 10

### 1.a
```{r Boston_explore_1}
library(MASS)
# head(Boston)
# ?Boston
str(Boston)
n <- dim(Boston)
cat("Number of rows = ", n[1], "\nNumber of columns", n[2], "\n")
```

There are 506 obs and 14 columns in the Boston dataset

This dataset contains information collected by the U.S CensusService concerning housing in the area of Boston, Massachusetts.
The rows represent different suburbs in Boston whereas the columns represent various characteristics of that area:

| Variable Name | Description                                |
| ------------- |:------------------------------------------:|
| crim          | per capita crime rate by town.
| zn            | proportion of residential land zoned for lots over 25,000 sq.ft.
| indus         | proportion of non-retail business acres per town.
| chas          | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
| nox           | nitrogen oxides concentration (parts per 10 million).
| rm            | average number of rooms per dwelling.
| age           | proportion of owner-occupied units built prior to 1940.
| dis           | weighted mean of distances to five Boston employment centres.
| rad           | index of accessibility to radial highways.
| tax           | full-value property-tax rate per \$10,000.
| ptratio       | pupil-teacher ratio by town.
| black         | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
| lstat         | lower status of the population (percent).
| medv          | median value of owner-occupied homes in \$1000s.

### 1.b
```{r Boston_explore_2, echo=FALSE}
pairs(Boston)
pairs(Boston[c("lstat","medv","nox","dis","age","rm")])
```

* lstat and medv have a negative relationship
* Houses with high medv have low crime rates (crim)
* Nitrogen oxide concentration (nox) reduces as distance from employment centres (dis) increases, but non-linearly.
* There is a linear relationship (few outliers) between medv and rm (no of rooms)

### 1.c
```{r, results="hide"}
par(mfrow = c(2, 3))
  
sapply(2:length(Boston), function(x)
  {
    plot(Boston[,c(1,x)], pch = 4, cex=0.7)
  }
)
```

Yes, there are predictors that are associated with crime rate.
* chas -> Most of the crimes occur away from the river. however this may be misleading as most of the recorded observations are away from the river
* age -> areas with high number of old houses tend to have higher crime rates
* dis -> crime rates are higher in areas which are closer to Boston's employment centres
* medv -> areas with more higher priced homes have lower crime rates. Some high price areas have high crime rates

### 1.d
```{r}
sortedBy_crime_rates <- Boston[order(Boston$crim, decreasing = T),]
sortedBy_crime_rates[1:6,]

sortedBy_tax <- Boston[order(Boston$tax, decreasing = T),]
sortedBy_tax[1:6,]

sortedBy_ptratio <- Boston[order(Boston$ptratio, decreasing = T),]
sortedBy_ptratio[1:6,]

summary(Boston)
```

381,419,406,411,415 are the suburbs with highest crime rates
489,490,491,492,493 are the suburbs with highest tax rates
355,356,128,129,130 are the suburbs with highest tax rates

crim rates range between 0.006 and 88.969
tax rates range between 187 and 711
Teacher-pupil ratios range between 12.6 and 22

### 1.e
```{r}
table(Boston$chas)
```

35 suburbs bound the Charles river

### 1.f
```{r}
summary(Boston$ptratio)
```

Median value is 19.05

### 1.g
```{r}
which.min(Boston$medv)
Boston[which.min(Boston$medv),]

```

the 399th suburb of Boston the lowest value of house prices

crim    zn indus  chas   nox    rm   age    dis  rad  tax
38.3518  0  18.1    0   0.693  5.453 100  1.4896  24  666
    ptratio black lstat  medv
      20.2  396.9  30.59   5

Crime rates - Crime rates are high and lie between the 3rd and 4th quartile
zn - there is no land zoned for large lots which is in the lowest point in the range with the lowest value
indus - indus value lies at the end of the 3rd quartile of the range
chas - It does not lie along the Charles river like most other suburbs
nox - it lies between the 3rd and 4th quartile of teh range
rm - it lies in the first quartile in the range
age - it is at the highest point of the range with the max value
dis - it lies in the first quartile of the range
rad - it is at the highest point of the range with the max value
tax (full-value property-tax rate per /$10,000.) -  is on the higher side, and lies at the third quaritle (666)
ptratio (pupil-teacher ratio) - is amongst the highest and lies at the third quartile of the dataset (20.20)
black (1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.) - is the maximum value in the dataset (396.9)
lstat - (lower status of the population in percent) is very high. Lies between 3rd quartile (16.95) and max (37.97)

### 1.h
```{r}
dim(Boston[which(Boston$rm > 7), ])[1]
dim(Boston[which(Boston$rm > 8), ])[1]

```

* The median of medv of these suburbs is much higher than the whole dataset
* Majority of these houses are are in areas of high crime rates
* These suburbs have lower percentage of people belonging to the lower status of the population
* There is a lower proportion of African Americans in these suburbs
* These suburbs have higher accessibility to radial highways than the overall dataset

## Question 2 - Problem 15  |  Chapter 3

### 2.a
```{r, results="hide"}
library (MASS)
Boston$index = NULL
        
lapply( Boston[,-1], function(x) {
  summary(lm(Boston$crim ~ x))
  }
)

par(mfrow = c(2, 2))
lapply( Boston[,-1], function(x) {
    model <- lm(Boston$crim ~ x)
    plot(x, Boston$crim)
    abline(model$coef[1],model$coef[2],col=2,lwd=2)
  }
)
```

There is some statistical significance observed in models using the following variables:
zn, indus, nox, rm, age, dis, rad, tax, ptratio, black, lstat, medv

### 2.b
```{r}
multilinear_model <- lm(crim ~ .,data=Boston)
summary(multilinear_model)

```

The variables that we can reject null hypothesis for are: (p<0.05)
zn, dis, rad, black, medv

### 2.c
```{r, results="hide"}
par(mfrow = c(2, 2))
coefficients = NULL
lapply( Boston[,-1], function(x) 
  {
    model <- lm(Boston$crim ~ x)
    coefficients <<- c(coefficients,model$coefficients[2])
    plot(x, Boston$crim)
    abline(model$coef[1],model$coef[2],col=2,lwd=2)
  }
      )
multi_coefficients <- multilinear_model$coefficients[2:14]

names <- NULL
lapply(seq_along(Boston[,-1]), function(x) 
  {
    names <<- names(Boston)
  }
      )
names(coefficients) <- names[2:14]

plot(coefficients, multi_coefficients, pch = 4, cex=0.7, col=2)
```

### 2.d
```{r}
lapply( Boston[,-1], function(x) 
  {
    x2 = x^2
    x3 = x^3
    summary(lm(Boston$crim ~ x + x2 + x3))
  }
)

```

The following variables have evidence of a nonlinear relationship:
indus, nox, dis, ptratio, medv

## Question 3 - Problem 9  |  Chapter 6

### 3.a
```{r Loading College}
library(caTools)
library(ISLR)
split = sample.split(College$Apps, SplitRatio = 0.7)
train = subset(College, split == TRUE)
test = subset(College, split == FALSE)

```
Train and test datasets were split into 70% and 30%

### 3.b
```{r linearModel on CollegeApps, echo=FALSE}
RMSE_hist_b <- NULL
# set.seed(1)

for(i in 1:5)
{
  set.seed(i)
  split = sample.split(College$Apps, SplitRatio = 0.7)
  train = subset(College, split == TRUE)
  test = subset(College, split == FALSE)
  model_collegeApp <- lm(Apps~., data = train)
  summary(model_collegeApp)
  
  # Making predictions and calculating
  predictions_collegeApp <- predict(model_collegeApp, newdata = test)
  SSE <- sum((predictions_collegeApp - test$Apps)^2)
  SST <- sum((mean(train$Apps) - test$Apps)^2)
  RMSE <- sqrt(SSE/nrow(test))
  RMSE_hist_b <- c(RMSE_hist_b, RMSE)
}
cat("The mean test error obtained was: ", round(mean(RMSE_hist_b),2), "\n")
```
The test error obtained using ordinary least squares regression was `r round(mean(RMSE_hist_b),2)`

### 3.c
```{r RidgeModel on College, echo=FALSE}
library(glmnet)
library(ISLR)

# checking for missing values
sapply(College, function(x) sum(is.na(x)))

x <- model.matrix(Apps~., data = College)[,-1]
y <- College$Apps
RMSE_hist_c <- NULL
# set.seed(1)

for(i in 1:5)
{
  set.seed(i)
  train = sample(1:nrow(x), nrow(x)/2)
  test = (-train)
  y.test=y[test]
  
  grid = 10^seq(10,-2, length = 100)
  ridge.mod = glmnet(x, y, alpha = 0, lambda = grid)
  cv.out = cv.glmnet(x[train, ], y[train], alpha = 0)
  plot(cv.out)
  bestlam = cv.out$lambda.min
  bestlam
  ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
  RMSE <- sqrt(mean((ridge.pred-y.test)^2))
  RMSE_hist_c <- c(RMSE_hist_c, RMSE)
}
cat("The test error with Ridge Regression was was:", round(mean(RMSE_hist_c),2), "with lambda =", bestlam,"\n")
```
The test error with Ridge Regression was was:`r round(mean(RMSE_hist_c),2)` with lambda = `r bestlam`

### 3.d
```{r LassoModel on College, echo=FALSE}
library(glmnet)
library(ISLR)

# checking for missing values
sapply(College, function(x) sum(is.na(x)))

x <- model.matrix(Apps~., data = College)[,-1]
y <- College$Apps
RMSE_hist_d <- NULL
# set.seed(1)

for(i in 1:5)
{
  set.seed(i)
  train = sample(1:nrow(x), nrow(x)/2)
  test = (-train)
  y.test=y[test]
  
  grid = 10^seq(8,-2, length = 100)
  lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
  plot(lasso.mod) # Drawing plots of coefficients
  
  cv.out = cv.glmnet(x[train, ], y[train], alpha = 1) # Fit lasso model on training data
  plot(cv.out) # Draw plot of training MSE as a function of lambda
  bestlam = cv.out$lambda.min # Select lamda that minimizes training MSE
  lasso.pred = predict(lasso.mod, s = bestlam, newx = x[test,]) # Use best lambda to predict test data
  RMSE <- sqrt(mean((lasso.pred - y.test)^2))
  RMSE_hist_d <- c(RMSE_hist_d, RMSE) 
}
cat("The test error with Lasso Regression was was:", round(mean(RMSE_hist_d),2), "with lambda =", bestlam,"\n")
```
The test error with Lasso model was was:`r round(mean(RMSE_hist_d),2)` with lambda = `r bestlam`

### 3.e
```{r PCR_model on College, echo=FALSE}
library(pls)
library(ISLR)

# checking for missing values
sapply(College, function(x) sum(is.na(x)))
# set.seed(1)
x <- model.matrix(Apps~., data = College)[,-1]
y <- College$Apps
RMSE_hist_e <- NULL
# set.seed(1)

for(i in 1:5)
{
  set.seed(i)
  train = sample(1:nrow(x), nrow(x)/2)
  test = (-train)
  y.test=y[test]
  
  pcr.fit = pcr(Apps~., data = College, subset = train, scale = TRUE, validation = "CV")
  summary(pcr.fit)
  validationplot(pcr.fit, val.type = "MSEP")
  # From the summary, it is obvious that the loweest cross-validation error occurs at `17 commponents, and stays the same on further increase in the number of components.
  pcr.pred = predict(pcr.fit, x[test,], ncomp = 17)
  RMSE <- sqrt(mean( (pcr.pred - y[test])^2 ))
  RMSE_hist_e <- c(RMSE_hist_e, RMSE)
}
cat("The test error with PCR Regression was was:", round(mean(RMSE_hist_e),2), "with m = 17\n")
```
The test error using Principal Component regression was was:`r round(mean(RMSE_hist_e),2)` with number of components = 17


### 3.f
```{r PLS_model on College, echo=FALSE}
library(pls)
library(ISLR)

# checking for missing values
sapply(College, function(x) sum(is.na(x)))
RMSE_hist_f <- NULL
# set.seed(1)

for(i in 1:5)
{
  set.seed(i)
  x <- model.matrix(Apps~., data = College)[,-1]
  y <- College$Apps
  train = sample(1:nrow(x), nrow(x)/2)
  test = (-train)
  y.test=y[test]
  
  pls.fit = plsr(Apps~., data = College, subset = train, scale = TRUE, validation = "CV")
  summary(pls.fit)
  validationplot(pls.fit, val.type = "MSEP")
  # From the summary, it is obvious that the loweest cross-validation error occurs at 8 commponents, and stays the same on further increase in the number of components.
  pls.pred = predict(pls.fit, x[test,], ncomp = 8)
  RMSE <- sqrt(mean( (pls.pred - y[test])^2 ))
  RMSE_hist_f <- c(RMSE_hist_f, RMSE)
}
cat("The test error with PLS Regression was was:", round(mean(RMSE_hist_f),2), "with m = 17\n")
```
The test error using Partial Least squares was was:`r round(mean(RMSE_hist_f))` with number of components = 8

### 3.g
| Regression type           | Root Mean Square error |
| ------------------------- |:----------------------:|
| Ordinary Least Squares    | `r round(mean(RMSE_hist_b))`
| Ridge regression          | `r round(mean(RMSE_hist_c))`
| Lasso regression          | `r round(mean(RMSE_hist_d))`
| PCR Model                 | `r round(mean(RMSE_hist_e))`
| PLS Model                 | `r round(mean(RMSE_hist_f))`

Since the average root mean square errors we have obtained in the models are between 1050 - 1350, our predictions are usually off by around a 1000 applications. The high RMSE can be explained because of some colleges taking in close to 48,000 applications (in which case an error rate of around a 1000 is reasonable), while some take in just 100.